{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNv57EyQH2oIf487ewrsH6W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JumanaRahim/Nullclass-Internship/blob/main/Gen_ai_model_for_image_colorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f3aI4fp-0XKg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.current_device())\n",
        "else:\n",
        "    print(\"No NVIDIA driver found. Using CPU\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElKEm2Xz1ej8",
        "outputId": "c3ea8366-8916-4bd1-bc30-725ad5125b8e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No NVIDIA driver found. Using CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform\n",
        ")\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=64, shuffle=True, num_workers=2\n",
        ")\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=64, shuffle=False, num_workers=2\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JI8cPeRR2xkj",
        "outputId": "c052afe9-24a6-4102-c643-a250320931fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 53.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ColorizationNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ColorizationNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=4, dilation=2)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=4, dilation=2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=4, dilation=2)\n",
        "        self.conv4 = nn.Conv2d(128, 3, kernel_size=5, stride=1, padding=4, dilation=2)  # Output 3 channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.relu(self.conv1(x))\n",
        "        x = nn.functional.relu(self.conv2(x))\n",
        "        x = nn.functional.relu(self.conv3(x))\n",
        "        x = torch.sigmoid(self.conv4(x))  # Output in range [0, 1]\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "d7aTQ0195Iuq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model and move it to the selected device (GPU or CPU)\n",
        "model = ColorizationNet().to(device)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.MSELoss()  # Mean Squared Error loss, often used for regression tasks\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001\n",
        "\n",
        "# Convert RGB image to grayscale\n",
        "def rgb_to_gray(img):\n",
        "    \"\"\"\n",
        "    Converts an RGB image tensor to grayscale by averaging the RGB channels.\n",
        "\n",
        "    Args:\n",
        "        img (torch.Tensor): Input tensor with shape (batch_size, 3, height, width)\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Grayscale tensor with shape (batch_size, 1, height, width)\n",
        "    \"\"\"\n",
        "    return img.mean(dim=1, keepdim=True)\n"
      ],
      "metadata": {
        "id": "WmOc6nsp6tt-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10  # Number of epochs\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0  # Track loss for the epoch\n",
        "\n",
        "    for step, (images, _) in enumerate(train_loader):\n",
        "        # Convert RGB images to grayscale and move to the device\n",
        "        grayscale_images = rgb_to_gray(images).to(device)\n",
        "        images = images.to(device)  # Original RGB images as targets\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(grayscale_images)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, images)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        loss.backward()        # Compute gradients\n",
        "        optimizer.step()       # Update model parameters\n",
        "\n",
        "        # Accumulate the loss for this step\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Print training status at regular intervals\n",
        "        if step % 50 == 0:  # Adjust the interval as needed\n",
        "            print(f\"Step [{step}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] completed. Average Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB8SFi_JkmTb",
        "outputId": "c5dc20be-af6a-4224-f1d0-939b97b036cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step [0/782], Loss: 0.0616\n",
            "Step [50/782], Loss: 0.0156\n",
            "Step [100/782], Loss: 0.0091\n",
            "Step [150/782], Loss: 0.0072\n",
            "Step [200/782], Loss: 0.0071\n",
            "Step [250/782], Loss: 0.0047\n",
            "Step [300/782], Loss: 0.0045\n",
            "Step [350/782], Loss: 0.0065\n",
            "Step [400/782], Loss: 0.0047\n",
            "Step [450/782], Loss: 0.0087\n",
            "Step [500/782], Loss: 0.0052\n",
            "Step [550/782], Loss: 0.0050\n",
            "Step [600/782], Loss: 0.0060\n",
            "Step [650/782], Loss: 0.0051\n",
            "Step [700/782], Loss: 0.0043\n",
            "Step [750/782], Loss: 0.0046\n",
            "Epoch [1/10] completed. Average Loss: 0.0078\n",
            "Step [0/782], Loss: 0.0061\n",
            "Step [50/782], Loss: 0.0058\n",
            "Step [100/782], Loss: 0.0043\n",
            "Step [150/782], Loss: 0.0050\n",
            "Step [200/782], Loss: 0.0046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EPOCHS = 10  # Number of epochs\n",
        "\n",
        "# for epoch in range(EPOCHS):\n",
        "#     for step, (images, _) in enumerate(train_loader):  # `images` is the input, `_` ignores labels (if any)\n",
        "#         # Convert RGB images to grayscale and move to the device\n",
        "#         grayscale_images = rgb_to_gray(images).to(device)\n",
        "#         images = images.to(device)  # Original RGB images as targets\n",
        "\n",
        "#         # Forward pass\n",
        "#         outputs = model(grayscale_images)\n",
        "\n",
        "#         # Compute the loss\n",
        "#         loss = criterion(outputs, images)\n",
        "\n",
        "#         # Backward pass and optimization\n",
        "#         optimizer.zero_grad()  # Clear gradients from the previous step\n",
        "#         loss.backward()        # Compute gradients\n",
        "#         optimizer.step()       # Update model parameters\n",
        "\n",
        "#         # Print training status at regular intervals\n",
        "#         if step % 10 == 0:\n",
        "#             print(f\"Epoch [{epoch+1}/{EPOCHS}], Step [{step}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "ey62Ztf2h71S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Function to display a single image\n",
        "def show(img):\n",
        "    img = img / 2 + 0.5  # Normalize the image for visualization\n",
        "    img = img.numpy()    # Convert tensor to numpy array\n",
        "\n",
        "    if len(img.shape) == 2:\n",
        "        plt.imshow(img, cmap='gray')  # Grayscale image\n",
        "    else:\n",
        "        plt.imshow(np.transpose(img, (1, 2, 0)))  # RGB image (HWC format)\n",
        "    plt.show()\n",
        "\n",
        "# Function to display original, grayscale, and colorized images side by side\n",
        "def visualize_all_three(original_images, grayscale_images, colorized_images, n=5):\n",
        "    fig = plt.figure(figsize=(15, 5))  # Adjust size based on number of images\n",
        "\n",
        "    for i in range(n):\n",
        "        # Original Image\n",
        "        ax = plt.subplot(n, 3, 3 * i + 1)\n",
        "        show(original_images[i])\n",
        "        ax.set_title(\"Original\")\n",
        "\n",
        "        # Grayscale Image\n",
        "        ax = plt.subplot(n, 3, 3 * i + 2)\n",
        "        show(grayscale_images[i])\n",
        "        ax.set_title(\"Grayscale\")\n",
        "\n",
        "        # Colorized Image\n",
        "        ax = plt.subplot(n, 3, 3 * i + 3)\n",
        "        show(colorized_images[i])\n",
        "        ax.set_title(\"Colorized\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "mLVgtlxRmDLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def rgb_to_hsv(image):\n",
        "    # Split the input RGB image into R, G, and B channels\n",
        "    r, g, b = image[..., 0], image[..., 1], image[..., 2]\n",
        "\n",
        "    # Compute max and min values for each pixel\n",
        "    max_val, _ = torch.max(image, dim=-1)  # Maximum of R, G, B\n",
        "    min_val, _ = torch.min(image, dim=-1)  # Minimum of R, G, B\n",
        "    diff = max_val - min_val               # Difference between max and min\n",
        "\n",
        "    # Initialize Hue (H) to zeros\n",
        "    h = torch.zeros_like(max_val)\n",
        "\n",
        "    # Compute Hue (H) based on the max channel\n",
        "    mask = (max_val == r) & (g >= b)       # Max is R and G >= B\n",
        "    h[mask] = (g[mask] - b[mask]) / diff[mask]  # Hue formula for R max\n",
        "    mask = (max_val == r) & (g < b)        # Max is R and G < B\n",
        "    h[mask] = (g[mask] - b[mask]) / diff[mask] + 6.0\n",
        "\n",
        "    mask = max_val == g                    # Max is G\n",
        "    h[mask] = (b[mask] - r[mask]) / diff[mask] + 2.0\n",
        "\n",
        "    mask = max_val == b                    # Max is B\n",
        "    h[mask] = (r[mask] - g[mask]) / diff[mask] + 4.0\n",
        "\n",
        "    h = h / 6.0  # Normalize Hue to the range [0, 1]\n",
        "    h[diff == 0.0] = 0.0  # Handle divide-by-zero cases\n",
        "\n",
        "    # Compute Saturation (S)\n",
        "    s = torch.zeros_like(max_val)\n",
        "    s[diff != 0.0] = diff[diff != 0.0] / max_val[diff != 0.0]  # S = diff / max\n",
        "    s[max_val == 0.0] = 0.0  # Avoid divide-by-zero when max_val is 0\n",
        "\n",
        "    # Value (V) is simply the max value\n",
        "    v = max_val\n",
        "\n",
        "    # Stack H, S, V into a single tensor along the last dimension\n",
        "    return torch.stack((h, s, v), dim=-1)\n"
      ],
      "metadata": {
        "id": "a4Z6UUFrm_k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def hsv_to_rgb(hsv):\n",
        "    # Split the HSV tensor into its components\n",
        "    h, s, v = hsv[..., 0], hsv[..., 1], hsv[..., 2]\n",
        "\n",
        "    # Scale the hue to [0, 6) for conversion\n",
        "    h = h * 6.0\n",
        "    i = torch.floor(h).int()  # Integer part of hue\n",
        "    f = h - i                # Fractional part of hue\n",
        "\n",
        "    # Precompute intermediate values\n",
        "    p = v * (1.0 - s)\n",
        "    q = v * (1.0 - f * s)\n",
        "    t = v * (1.0 - (1.0 - f) * s)\n",
        "\n",
        "    # Wrap around the hue index to fit within 0-5\n",
        "    i_mod = i % 6\n",
        "\n",
        "    # Create tensors for R, G, B components\n",
        "    r = torch.zeros_like(h)\n",
        "    g = torch.zeros_like(h)\n",
        "    b = torch.zeros_like(h)\n",
        "\n",
        "    # Map the intermediate values to RGB channels based on the hue sector\n",
        "    r[i_mod == 0] = v[i_mod == 0]\n",
        "    g[i_mod == 0] = t[i_mod == 0]\n",
        "    b[i_mod == 0] = p[i_mod == 0]\n",
        "\n",
        "    r[i_mod == 1] = q[i_mod == 1]\n",
        "    g[i_mod == 1] = v[i_mod == 1]\n",
        "    b[i_mod == 1] = p[i_mod == 1]\n",
        "\n",
        "    r[i_mod == 2] = p[i_mod == 2]\n",
        "    g[i_mod == 2] = v[i_mod == 2]\n",
        "    b[i_mod == 2] = t[i_mod == 2]\n",
        "\n",
        "    r[i_mod == 3] = p[i_mod == 3]\n",
        "    g[i_mod == 3] = q[i_mod == 3]\n",
        "    b[i_mod == 3] = v[i_mod == 3]\n",
        "\n",
        "    r[i_mod == 4] = t[i_mod == 4]\n",
        "    g[i_mod == 4] = p[i_mod == 4]\n",
        "    b[i_mod == 4] = v[i_mod == 4]\n",
        "\n",
        "    r[i_mod == 5] = v[i_mod == 5]\n",
        "    g[i_mod == 5] = p[i_mod == 5]\n",
        "    b[i_mod == 5] = q[i_mod == 5]\n",
        "\n",
        "    # Combine R, G, B into a single tensor\n",
        "    return torch.stack([r, g, b], dim=-1)\n"
      ],
      "metadata": {
        "id": "Vd8S3Vwkn-r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exaggerate_colors(images, saturation_factor=1.5, value_factor=1.2):\n",
        "    images = (images + 1) / 2.0\n",
        "    images_hsv = torch_rgb_to_hsv(images)\n",
        "    images_hsv[:, 1, :, :] = torch.clamp(images_hsv[:, 1, :, :] * saturation_factor, 0, 1)\n",
        "    images_hsv[:, 2, :, :] = torch.clamp(images_hsv[:, 2, :, :] * value_factor, 0, 2)\n",
        "\n",
        "    color_exaggerated_images = torch_hsv_to_rgb(images_hsv)\n",
        "    color_exaggerated_images = color_exaggerated_images * 2.0 - 1.0\n",
        "\n",
        "    return color_exaggerated_images\n"
      ],
      "metadata": {
        "id": "ntIxl-wG4CkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    for i, (images, _) in enumerate(test_loader):\n",
        "        grayscale_images = rgb_to_gray(images).to(device)\n",
        "        colorized_images = model(grayscale_images)\n",
        "\n",
        "        grayscale_images_cpu = grayscale_images.cpu().squeeze(1)\n",
        "        colorized_images_cpu = colorized_images.cpu()\n",
        "        original_images_cpu = images.cpu()\n",
        "\n",
        "        colorized_images_cpu = exaggerate_colors(colorized_images_cpu)\n",
        "\n",
        "        visualize_all_three(original_images_cpu, grayscale_images_cpu, colorized_images_cpu)\n",
        "\n",
        "        if i == 10:\n",
        "            break\n"
      ],
      "metadata": {
        "id": "r0Y1k3cGG3tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "img_tensor = transform(gray_img).unsqueeze(0)\n",
        "\n",
        "model.eval()\n",
        "img_tensor = img_tensor.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    colorized_tensor = model(img_tensor)\n",
        "\n",
        "colorized_img = transforms.ToPILImage()(colorized_tensor.squeeze(0).cpu())\n",
        "colorized_img.save(\"Colorized_Image.png\")\n"
      ],
      "metadata": {
        "id": "nmzAiN81G5EY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "ax[0].imshow(img)\n",
        "ax[0].set_title(\"Original Color Image\")\n",
        "ax[0].axis(\"off\")\n",
        "\n",
        "ax[1].imshow(gray_img, cmap=\"gray\")\n",
        "ax[1].set_title(\"Grayscale Image\")\n",
        "ax[1].axis(\"off\")\n",
        "\n",
        "ax[2].imshow(colorized_img)\n",
        "ax[2].set_title(\"Colorized Image\")\n",
        "ax[2].axis(\"off\")  # Hide axes\n",
        "\n",
        "plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "per_thP-1LTq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}